---
layout: post
title: Statistical Learning Theory
published: true
usemathjax: true
tags: theory
---

Statistical learning theory gives us a framework to quantify the error of a predictive model. Specifically, given a predictive model $$f$$, labeled dataset which is distributed iid from an (unknown) joint distribution $$P$$ on $$\mathcal{X} \times \mathcal{Y}$$, and a loss function $$\mathcal: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$$ bounded below by some constant.

To begin, we are concerned with studying the expected risk,

$$ \begin{equation}
L(f) = \mathbb{E}_{(x,y) \sim P} \[(f(x)-y)^2\]
\end{equation} $$

I.e., under certain conditions and assumptions on the data distribution, model structure (hypothesis class), what are statements (asymptotic or exact) we can make about the expected risk.

The first lemma is called Decomposition of Loss. It gives us a way to separate the intrinsic variation of the label, $$y$$, from our error. 

Lemma 1: Under the squared loss we have

$$ \begin{equation}
L(f) = \mathbb{x \sim P_x} \[ (f(x)-\mathbb{E}[y \vert x])^2\ \] + \mathbb{x \sim P_x} \[Var(y \vert x)\]
\end{equation} $$

We can immediately see that the optimal model is $$f(x) = \mathbb{E} \[ y \vert x \]$$.

Proof of Lemma 1. We have

$$ \begin{align*}
L(f) &= \mathbb{E}\[(f(x)-y)^2\] \\
&= \mathbb{E}_{x \sim P_x}\[ \mathbb{E}_{P_{y \vert x}} \[ (f(x)-y)^2 \vert x \] \] \\
&= \mathbb{x \sim P_x} \[ (f(x)-\mathbb{E}[y \vert x])^2\ \] + \mathbb{x \sim P_x} \[Var(y \vert x)\] \\
\end{align*} $$

Using the Law of Total Expectation and another CLAIM (to prove.)

Lemma 1 gives us a generic lower bound, but we can often make this lower bound tighter if we know the structure of the hypothesis class $$\mathcal{F}$$ from which we learn $$f$$.

#### Assumption (Choice) 1: Linear Hypothesis Class / Linear Regression Under Squared Loss

The loss is then 

$$ \begin{equation}
L(f) = L(w) = \mathbb{E}[(w^Tx-y)^2]
\end{equation} $$

As a shorthand let $$w^* \in argmin_{w} L(w)$$

##### Lemma 2: For linear models, we can separate out the error inherent in linear models and error incurred from a finite sample

Statement.

$$ \begin{equation}
L(\hat{w}) = \mathbb{E}_{x} \[ Var(y \vert x) \] + \mathbb{E}_{x} \[ (\mathbb{E}\[y \vert x\] = w^{*}^Tx)^2 \] + \mathbb{E}_{x}\[(w^{*}^T - \hat{w}^T x)^2 \]
\end{equation} $$

The second term can be thought of as approximation error inherent in linear models, and the thrid term is estimation error for a finite sample. So linear regression error equals intrinsic variable error plus linear model error plus finite approximation error.

Proof. Let $$g(\hat{w}) = \coloneqq \mathbb{E} \[ (\mathbb{E}\[y \vert x\] - \hat{w}^Tx)^2 \]$$. By Lemma 1,

$$ \begin{equation}
L(\hat{w}) = \mathbb{E} \[ Var(y \vert x) \] + g(\hat{w})
\end{equation} $$

Since $$\mathbb{E}_{x} \[ Var(y \vert x)\]$$ is a constant wrt $$w$$, we have

To-Do: Proof of Lemma 2. 

#### Parameterized families of hypotheses

In general, a parameterized family is given by a parameter space. For each $$\theta$$ in the space, there exists a hypothesis, written $$f_{\theta}(x)$$ or $$f(\theta;x)$$.

##### Well-specified case and maximum likelihood

