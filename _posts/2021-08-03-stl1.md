---
layout: post
title: Statistical Learning Theory
published: true
usemathjax: true
tags: theory
---

Statistical learning theory gives us a framework to quantify the error of a predictive model. Specifically, given a predictive model $$f$$, labeled dataset which is distributed iid from an (unknown) joint distribution $$P$$ on $$\mathcal{X} \times \mathcal{Y}$$, and a loss function $$\mathcal: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$$ bounded below by some constant.

Although SLT is ultimately limited in the sense that real world machine learning generally does not coincide with the iid assumption, and many other mathematical assumptions used in SLT, it is a starting point to form the study of machine learning. In recent years, the field of deep learning theory has gained visibility as an attempt to formalize the behavior of deep neural networks. I am embarking on an endeavor to learn SLT as it will give me the concepts and language to approach deep learning theory, as well has help me gain more intuition for ML, optimization, and statistics problems in a wide variety of contexts.

To begin, we are concerned with studying the expected risk,
<!--excerpt-->

$$ \begin{equation}
L(f) = \mathbb{E}_{(x,y) \sim P} [(f(x)-y)^2]
\end{equation} $$

I.e., under certain conditions and assumptions on the data distribution, model structure (hypothesis class), what are statements (asymptotic or exact) we can make about the expected risk.

The first lemma is called Decomposition of Loss. It gives us a way to separate the intrinsic variation of the label, $$y$$, from our error. 

Lemma 1: Under the squared loss we have

$$ \begin{equation}
L(f) = \mathbb{E}_{x \sim P_x} [ (f(x)-\mathbb{E}[y \vert x])^2\ \] + \mathbb{E}_{x \sim P_x} [Var(y \vert x)]
\end{equation} $$

We can immediately see that the optimal model is $$f(x) = \mathbb{E} \[ y \vert x \]$$.

Proof of Lemma 1. We have

$$ \begin{align*}
L(f) &= \mathbb{E}[(f(x)-y)^2] \\
&= \mathbb{E}_{x \sim P_x}[ \mathbb{E}_{P_{y \vert x}} [ (f(x)-y)^2 \vert x ] ] \\
&= \mathbb{E}_{x \sim P_x} [ (f(x)-\mathbb{E}[y \vert x])^2 ] + \mathbb{E}_{x \sim P_x} [Var(y \vert x)] \\
\end{align*} $$

Using the Law of Total Expectation and another CLAIM (to prove.)

Lemma 1 gives us a generic lower bound, but we can often make this lower bound tighter if we know the structure of the hypothesis class $$\mathcal{F}$$ from which we learn $$f$$.

#### Assumption (Choice) 1: Linear Hypothesis Class / Linear Regression Under Squared Loss

The loss is then 

$$ \begin{equation}
L(f) = L(w) = \mathbb{E}[(w^Tx-y)^2]
\end{equation} $$

As a shorthand let $$w^* \in argmin_{w} L(w)$$

##### Lemma 2: For linear models, we can separate out the error inherent in linear models and error incurred from a finite sample

Statement.

$$ \begin{equation}
L(\hat{w}) = \mathbb{E}_{x} [ Var(y \vert x) ] + \mathbb{E}_{x} [ (\mathbb{E}[y \vert x] - w^{*}^Tx)^2 ] + \mathbb{E}_{x}[(w^{*}^T - \hat{w}^T x)^2 ]
\end{equation} $$

The second term can be thought of as approximation error inherent in linear models, and the thrid term is estimation error for a finite sample. So sqaured linear regression error equals intrinsic variable error plus linear model error plus finite approximation error.

Proof. Let $$g(\hat{w}) = \coloneqq \mathbb{E} [ (\mathbb{E}[y \vert x] - \hat{w}^Tx)^2 ]$$. By Lemma 1,

$$ \begin{equation}
L(\hat{w}) = \mathbb{E} [ Var(y \vert x) ] + g(\hat{w})
\end{equation} $$

Since $$\mathbb{E}_{x} [ Var(y \vert x)]$$ is a constant wrt $$w$$, we have

To-Do: Proof of Lemma 2. 

#### Parameterized families of hypotheses

In general, a parameterized family is given by a parameter space. For each $$\theta$$ in the space, there exists a hypothesis, written $$f_{\theta}(x)$$ or $$f(\theta;x)$$.

##### Well-specified case and maximum likelihood

In the well specified case, we can say that $$y \vert x \sim P_{\theta^*} (y \vert x)$$ is distributed according to some ground truth parameter $$\theta^*$$. Our hypothesis space is $$P_{\theta}(y \vert x)$$, a family of distributions parameterized by $$\theta \in \Theta$$.

##### Metric 2: Maximum Likelihood Loss Function

(Note that metric 1 was sqaured loss.)

Define the maximum likelihood loss function by

$$ \begin{equation}
\mathcal{l}((x,y),\theta) = - \log P_{\theta}(y \vert x)
\end{equation} $$

So minimizing the loss function is equivalent to maximizing the likelihood of the data. As an example, let $$y \vert x$$ by a Gaussian distributed with mean $$\theta^{*}^T x$$ and variance $$1$$, i.e. suppose $$y \vert x \sim N(\theta^{*}^T x,1)$$. The likelihood loss is then

$$ \begin{align*}
\mathcal{l}((x,y), \theta) &=  - \log P_{\theta} (y \vert x) \\
&= - \log exp \big( - \dfrac{(y - \theta^x)^2}{2} \big) + c \\
&= \dfrac{(y - \theta^x)^2}{2} + c
\end{align*} $$

Where $$c$$ is the log of the normalizing constant. 

##### Training Loss

We need to approximate the training loss using samples since often we do not know the true underlying distribution $$P$$. Define the training loss as

$$
\hat{L}(\theta) \coloneqq \frac{1}{n} \sum_{i=1}^n \mathcal{l}((x^{i},y^{i}), \theta)
$$

In the special case of maximum likelihood, we plug in to get the *sample maximum likelihood loss* aka *training loss*

$$
\begin{equation}
\hat{L}(\theta) \coloneqq - \frac{1}{n} \sum_{i=1}^n \log p_{\theta}(y \vert x)
\end{equation}
$$

And define the *maximum likelihood estimator*

$$ \begin{equation}
\hat{theta}_{MLE} \in argmin_{\theta \in \Theta} \hat{L}(\theta)
\end{equation} $$

This approximation is "good" in the asympotic sense. Below, we quantify this statement.

#### Theorem 1: Asymptotic of MLE

##### Assume

1. That $$\nabla^2 L(\theta^*)$$ is full rank, i.e. the Hessian of the loss is a full rank matrix
2. $$\hat{theta} = \theta_{MLE}$$ is a consistent estimator. (As the number of data points, $$n$$, approaches infinity, the minimizer of the training loss $$\hat{\theta}_{MLE}$$ converges to the true maximum likelihood parameter $$theta^*$$, i.e. the true $$P_{\theta^*}(y \vert x)$$.)

##### Let
$$ \begin{equation}
Q \coloneqq \mathbb{E}_{(x,y) \sim P} [\nabla_{\theta}(\log p_{\theta}(y \vert x))(\theta^*) \nabla_{\theta}(\log p_{\theta}(y \vert x))(\theta^*)^T].
\end{equation} $$
I.e. $$Q$$ is the Fisher information matrix

##### Then

1. $$\sqrt{n}(\hat{\theta} - \theta^*) \rightarrow N(0, Q^{-1})$$
2. $$n(L(\hat{\theta}) - L(\theta^*)) \rightarrow \frac{1}{2} \chi^2(p)$$

Where $\chi^2(p)$ is the chi-square distribution

### Resources

1. CS229T notes by Percy Liang and Tengyu MA
2. https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties
3. https://en.wikipedia.org/wiki/Consistent_estimator

### Next

1. Proof of the Asymptotics of MLE, touching on uniform convergence, identifiability 
