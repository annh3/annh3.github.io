---
layout: post
title: Statistical Learning Theory
published: true
usemathjax: true
tags: theory
---

Statistical learning theory gives us a framework to quantify the error of a predictive model. Specifically, given a predictive model $$f$$, labeled dataset which is distributed iid from an (unknown) joint distribution $$P$$ on $$\mathcal{X} \times \mathcal{Y}$$, and a loss function $$\mathcal: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$$ bounded below by some constant.

To begin, we are concerned with studying the expected risk,

$$ \begin{equation}
L(f) = \mathbb{E}_{(x,y) \sim P} \[(f(x)-y)^2\]
\end{equation} $$

I.e., under certain conditions and assumptions on the data distribution, model structure (hypothesis class), what are statements (asymptotic or exact) we can make about the expected risk.

The first lemma is called Decomposition of Loss. It gives us a way to separate the intrinsic variation of the label, $$y$$, from our error. 

Lemma 1: Under the squared loss we have

$$ \begin{equation}
L(f) = \mathbb{x \sim P_x} \[ (f(x)-\mathbb{E}[y \vert x])^2\ \] + \mathbb{x \sim P_x} \[Var(y \vert x)\]
\end{equation} $$

We can immediately see that the optimal model is $$f(x) = \mathbb{E} \[ y \vert x \]$$.

Proof of Lemma 1. We have

$$ \begin{align*}
L(f) &= \mathbb{E}\[(f(x)-y)^2\] \\
&= \mathbb{E}_{x \sim P_x}\[ \mathbb{E}_{P_{y \vert x}} \[ (f(x)-y)^2 \vert x \] \] \\
&= \mathbb{x \sim P_x} \[ (f(x)-\mathbb{E}[y \vert x])^2\ \] + \mathbb{x \sim P_x} \[Var(y \vert x)\] \\
\end{align*} $$

Using the Law of Total Expectation and another CLAIM (to prove.)

Lemma 1 gives us a generic lower bound, but we can often make this lower bound tighter if we know the structure of the hypothesis class $$\mathcal{F}$$ from which we learn $$f$$.
