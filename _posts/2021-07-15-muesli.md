---
layout: post
title: Policy Gradients Part II: Muesli
published: false
usemathjax: true
tags: policy gradients
---

Use the Maximum a Posteriori Policy Optimization (MPO) algorithm (Abdolmaleki 2018) as a starting point, since (1) it can learn stochastic policies, (2) supports discrete and continuous action spaces, (3) can learn stably from off-policy data, and has strong performance bounds even when using approximate Q-values.

MPO Paper

* Based on coordinate ascent on a relative entropy objective
* We show that several existing methods can be directly related to our derivation
* Develop two off-policy algorihtms and demosntrate that they are competitive with the sota in deep RL
* For continuous control, our method outperforms existing methods with respect to (a) sample efficiency, (b) premature convergence, and (c) robustness to hyperparameter settings.

* Two types of algorithms currently dominate scalable learning for continuous control problems: TRPO and PPO
* These policy-gradient algorithms are on-policy by design, reducing gradient variance through large batches and limitng the allowed change in parameters
* They are robust, applicable to high-dimensional problems, and require moderate paramter tuning
* Hyperparameter insensitive
* As they are on-policy, they suffer from poor sample efficiency

* Off-policy value-gradient algorithms such as DDPG, SVG, NAF use experience replay 
* Much better data efficiency, approaching the regime where experiments with real robots are possible
* Difficult to tune

* To derive our algorithm, we take advantage of the duality between control and estimation by using EM, a powerful tool from the probabilistic estimation toolbox, in order to solve control problems
* This duality can be understood as replacing the question "what are the actions which maximize future rewards?" with the question "assuming future success in maximizing rewards, what are the actions most likely to have been taken?"
* We show that several algorithms, including TRPO, can be directly related to this perspective
* We leverage the fast convergence properties of EM-style coordinate ascent
* E: data-based, re-weights state-action samples
* M: supervised, parametric M-step using DNN

* In contrast to typical off-policy value-gradient algorithms, the new algorithm does not require gradient of the Q-function to udpate the policy
* Instead, it uses samples from the Q-function to compare different actiosn in a given state

Experiments:
* 56 DoF humanoid body

Background:
* Casting RL as an inference problem
* Variational inference perspective on RL
* Obtaining maximum entropy policies as the solution to an inference problem
* The penalization of determinism can be seen encouraging both robustness and simplicity
* "RL as inference" - use entropy or KL divergence as regularizers
* Trust-region based methods can be seen as optimizing a parametric E-step, but missing an explicit M step
* The connection between RL and inference has been invoked to motivate work on exploration
* 




