---
layout: post
title: Expectation Maximization, Variational Inference, TRPO
published: false
usemathjax: true
tags: machine learning
---

The EM algorithm is a generic optimization algorithm for cases where latent variables make the optimization problem difficult. It is an alternating optimization
algorithm based on Jensen's inequality. This post will be concerned with deriving the EM algorithm from first principles, exploring use cases, such as optimizing mixtures of Gaussians, relating it to variational inference, and showing how it is conceptually similar to Trust Region Policy Optimization.

Let's review some definitions and theorems.

#### Convexity

$$\begin{equation}
f \text{is convex} \leftrightarrow f''(x) \ge 0 \forall x \in \text{domain}
\end{equation}$$

If the domain of $$x$$ is multivariate this is equivalent to saying that the Hessian $$H$$ of $$f$$ is positive semi-definite. If f'' is positive at all $$x$$, then 
$$f$$ is strictly convex.

#### Theorem: Jensen's Inequality

If $$f$$ is convex and $$X$$ is a random variable, then 

$$ \begin{equation}
\mathbb{E}[f(X)] \ge f(\mathbb{E}[X])
\end{equation} $$

When $$f$$ is strictly convex, then equality occurs iff $$E[X] = X$$, i.e. the random variable $$X$$ is a constant. Jensen's says that, for $$f$$ convex, the mean of $$f(x)$$ is lower bounded by $$f$$ applied to the mean of $$X$$.

#### Deriving the EM Algorithm

Say that we have a latent variable model $$p(x,z;\theta)$$ with $$z$$ being the latent variable. Since we only have access to $$x$$, we can rewrite the model as the density for $$x$$ by marginalizing over $$z$$.

$$ \begin{equation}
p(x; \theta) = \sum_{z} p(x,z;\theta)
\end{equation} $$

And fit the parameters of the model, $$\theta$$, by maximizing the log-likelihood of the data $$x$$,

$$ \begin{align*}
l(\theta) &= \sum_{i=1}^n \log p(x^{(i)}; \theta) \\
&= \sum_{i=1}^n \log \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) \\
\end{align*} $$

Oftentimes, when the latent $$z^{(i)}$$'s are fully observed, the $$MLE$$ is easy. In the latent variable setting, $$EM$$ is an efficient $$MLE$$ algorithm. The strategy of the $$EM$$ algorithm is to construct a lower bound (E-step) and then optimize the lower bound (M-step). To derive the EM algorithm, let's restrict our attention to the single data point case, where we are optimizing $$\log p(x)$$.

$$ \begin{align*}
\log p(x; \theta) &= \log \sum_{z} p(x,z;\theta) \\
&= \log \sum_{z} Q(z) \dfrac{p(x,z;\theta)}{Q(z)} \\
& \ge \sum_{z} Q(z) \log \dfrac{p(x,z;\theta)}{Q(z)} \\
\end{align*} $$

Where we apply Jensen's equality to $$log$$, a concave function. (The direction of Jensen's is flipped for concave fucntions.)

The lower bound works for arbitrary distributions $$Q$$ but we would like the select $$Q$$ such that the bound is tight, thereby giving us a more accurate estimate.
Note that $$\sum_{z} Q(z) \log \dfrac{p(x,z;\theta)}{Q(z)}$$ is the form of an expectation. For the bound to be tight, we simply need $$\dfrac{p(x,z;\theta)}{Q(z)}$$ to be a constant random variable, so we force this by choosing

$$ \begin{equation}
Q(z) \propto p(x,z;\theta)
\end{equation} $$

Giving us $$\dfrac{p(x,z;\theta)}{Q(z)} = C$$ and 

$$ \begin{align*}
Q(z) &= \dfrac{p(x,z;\theta)}{\sum_{z} p(x,z;\theta)} \\
&= \dfrac{p(x,z;\theta)}{p(x;\theta)} \\
&= p(z \vert x;\theta) \\
\end{align*} $$

To simplify our notation going forward, we let

$$ \begin{equation}
ELBO(x;Q,\theta) = \sum_{z} Q(z) \log \dfrac{p(x,z;\theta)}{Q(z)}
\end{equation} $$

The EM algorithm alternates between:

(a) setting $$Q(z0 = p(z \vert x; \theta)$$
(b) maximizing $$ELBO(x;Q,\theta)$$ wrt $$\theta$$ while $$Q$$ is fixed

#### Application: EM for Mixtures of Gaussians



#### Deriving the EM Update for Mixtures of Gaussians

We'll formally derive the udpate for a covariance matrix, noting that the update for a mean vector is similar. To maximize ELBO wrt $$\Sigma_{l}$$, do

$$ \begin{align*}
\nabla_{\Sigma_{l}} \sum_{i=1}^n \sum_{j=1}^k w_{j}^{(i)} \log \dfrac{1}{(2 \pi)^{d/2} \vert \Sigma_{j} \vert^{1/2}} exp(\dfrac{\frac{-1}{2} (x^{(i)} - \mu_{j})^{T} \Sigma_{j}^{-1} (x^{(i)} - \mu_{j}) \cdot \phi_{j}}{w_{j}^{(i)}})
&=  \nabla_{\Sigma_{l}} \sum_{i=1}^n \sum_{j=1}^k w_{j}^{(i)} \big[  \log \dfrac{1}{(2 \pi)^{d/2} \vert \Sigma_{j} \vert^{1/2}} + \log \phi_{j} - \log w_{j}^{(i)} - \dfrac{1}{2} (x^{(i)} - \mu_{j})^{T} \Sigma_{j}^{-1} (x^{(i)} - \mu_{j})  \big] \\
&=  \sum_{i=1}^n w_{l}^{(i)} \big[ \nabla_{\Sigma_{l}} \log \dfrac{1}{(2 \pi)^{d/2} \vert \Sigma_{j} \vert^{1/2}} -  \nabla_{\Sigma_{l}} \dfrac{1}{2} (x^{(i)} - \mu_{j})^{T} \Sigma_{j}^{-1} (x^{(i)} - \mu_{j}) \big] \\
&= \sum_{i=1}^n  w_{l}^{(i)} \big[ \dfrac{-1}{2} Sigma_{l}^{-1} + \dfrac{1}{2} \Sigma_{l}^{-T} (x^{(i)} - \mu_{j})(x^{(i)} - \mu_{j})^{T} \Sigma_{l}^{-T} \big] \\
\end{align*} $$

Setting to $$0$$ and solving gives the update

$$\Sigma_{l} \coloneqq \dfrac{\sum_{i=1}^n w_{l}^{(i)} (x^{(i)} - \mu_{j})(x^{(i)} - \mu_{j})^{T}}{\sum_{i=1}^n w_{l}^{(i)} }$$. We used a couple of identities from the Matrix Cookbook to help derive our update. For example, $$\frac{d}{d \Sigma} a^{T} \Sigma^{-1} a = - \Sigma^{-T} a a^T \Sigma^{-T}$$. Futhermore, we assume orthogonality of the covariance matrix, giving us $$\Sigma_{l}^{-T} = \Sigma_{l}^{-1}$$.

Let's furthermore derive the update for $$\phi_{j}$$.

#### Implementing EM for Mixtures of Gaussians

#### KL Divergence Interpretation

#### Variational Inference

The term Variational Auto-Encoder generally refers to a family of algorithms that extend the EM algorithms to models parameterized by neural networks. Specifically, VAE extends teh techniques of variational inference using the "reparameterization trick." For many datasets, VAE does not achieve top perforamance but is important because it demonstrates central ideas about how to extend EM algorithms to high-dimensional continuous models with latent variables.

#### Connection to TRPO
