---
layout: post
title: Expectation Maximization Algorithm
published: false
usemathjax: true
tags: machine learning
---

The EM algorithm is a generic optimization algorithm for cases where latent variables make the optimization problem difficult. It is an alternating optimization
algorithm based on Jensen's inequality. This post will be concerned with deriving the EM algorihtm from first principles, exploring use cases, such as optimizing mixtures of Gaussians, relating it to variational inference, and showing how it is conceptually similar to Trust Region Policy Optimization.

Let's review some definitions and theorems.

#### Convexity

$$\begin{equation}
f \text{is convex} \leftrightarrow f''(x) \ge 0 \forall x \in \text{domain}
\end{equation}$$

If the domain of $$x$$ is multivariate this is equivalent to saying that the Hessian $$H$$ of $$f$$ is positive semi-definite. If f'' is positive at all $$x$$, then 
$$f$$ is strictly convex.

#### Theorem: Jensen's Inequality

If $$f$$ is convex and $$X$$ is a random variable, then 

$$ \begin{equation}
\mathbb{E}[f(X)] \ge f(\mathbb{E}[X])
\end{equation} $$

When $$f$$ is strictly convex, then equality occurs iff $$E[X] = X$$, i.e. the random variable $$X$$ is a constant. 

#### Deriving the EM Algorithm



