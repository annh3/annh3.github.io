---
layout: post
title: Expectation Maximization Algorithm
published: false
usemathjax: true
tags: machine learning
---

The EM algorithm is a generic optimization algorithm for cases where latent variables make the optimization problem difficult. It is an alternating optimization
algorithm based on Jensen's inequality. This post will be concerned with deriving the EM algorihtm from first principles, exploring use cases, such as optimizing mixtures of Gaussians, relating it to variational inference, and showing how it is conceptually similar to Trust Region Policy Optimization.

Let's review some definitions and theorems.

#### Convexity

$$\begin{equation}
f \text{is convex} \leftrightarrow f''(x) \ge 0 \forall x \in \text{domain}
\end{equation}$$

If the domain of $$x$$ is multivariate this is equivalent to saying that the Hessian $$H$$ of $$f$$ is positive semi-definite. If f'' is positive at all $$x$$, then 
$$f$$ is strictly convex.

#### Theorem: Jensen's Inequality

If $$f$$ is convex and $$X$$ is a random variable, then 

$$ \begin{equation}
\mathbb{E}[f(X)] \ge f(\mathbb{E}[X])
\end{equation} $$

When $$f$$ is strictly convex, then equality occurs iff $$E[X] = X$$, i.e. the random variable $$X$$ is a constant. 

#### Deriving the EM Algorithm

Say that we have a latent variable model $$p(x,z;\theta)$$ with $$z$$ being the latent variable. Since we only have access to $$x$$, we can rewrite the model as the density for $$x$$ by marginalizing over $$z$$.

$$ \begin{equation}
p(x; \theta) = \sum_{z} p(x,z;\theta)
\end{equation} $$

And fit the parameters of the model, $$\theta$$, by maximizing the log-likelihood of the data $$x$$,

$$ \begin{align*}
l(\theta) &= \sum_{i=1}^n \log p(x^{(i)}; \theta) \\
&= \sum_{i=1}^n \log \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) \\
\end{align*} $$

Oftentimes, when the latent $$z^{(i)}$$'s are fully observed, the $$MLE$$ is easy. In the latent variable setting, $$EM$$ is an efficient $$MLE$$ algorithm. The strategy of the $$EM$$ algorithm is to construct a lower bound (E-step) and then optimize the lower bound (M-step). To derive the EM algorithm, let's restrict our attention to the single data point case, where we are optimizing $$\log p(x)$$
