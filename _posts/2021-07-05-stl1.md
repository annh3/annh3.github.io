---
layout: post
title: Statistical Learning Theory (I)
published: true
usemathjax: true
tags: theory
---

Statistical learning theory gives us a framework to quantify the error of a predictive model. Specifically, given a predictive model $$f$$, labeled dataset which is distributed *iid* from an (unknown) joint distribution $$P$$ on $$\mathcal{X} \times \mathcal{Y}$$, and a loss function $$\mathcal{l}: \mathcal{Y} \times \mathcal{Y} \rightarrow \mathbb{R}$$ bounded below by some constant, how does $$f$$ fare when measured against ground truth labels?

Although SLT is ultimately limited in the sense that real world machine learning generally does not coincide with the *iid* assumption, and many other mathematical assumptions used in SLT, it is a starting point to form the study of machine learning. In recent years, the field of deep learning theory has gained visibility as an attempt to formalize the behavior of deep neural networks. I am embarking on an endeavor to learn SLT as it will give me the concepts and language to approach deep learning theory, as well has help me gain more intuition for ML, optimization, and statistics problems in a wide variety of contexts.

To begin, we are concerned with studying the expected risk,
<!--excerpt-->

$$ \begin{equation}
L(f) = \mathbb{E}_{(x,y) \sim P} [(f(x)-y)^2]
\end{equation} $$

I.e., under certain conditions and assumptions on the data distribution, model structure (hypothesis class), what are statements (asymptotic or exact) we can make about the expected risk.

The first lemma is called Decomposition of Loss. It gives us a way to separate the intrinsic variation of the label, $$y$$, from our error. 

Lemma 1: Under the squared loss we have

$$ \begin{equation}
L(f) = \mathbb{E}_{x \sim P_x} [ (f(x)-\mathbb{E}[y \vert x])^2 ] + \mathbb{E}_{x \sim P_x} [Var(y \vert x)] 
\end{equation} $$

We can immediately see that the optimal model is $$f(x) = \mathbb{E} [ y \vert x ]$$.

Proof of Lemma 1. We have

$$ \begin{align*}
L(f) &= \mathbb{E}[(f(x)-y)^2] \\
&= \mathbb{E}_{x \sim P_x}[ \mathbb{E}_{P_{y \vert x}} [ (f(x)-y)^2 \vert x ] ] \\
&= \mathbb{E}_{x \sim P_x} [ (f(x)-\mathbb{E}[y \vert x])^2 ] + \mathbb{E}_{x \sim P_x} [Var(y \vert x)] \\
\end{align*} $$

Lemma 1 gives us a generic lower bound, but we can often make this lower bound tighter if we know the structure of the hypothesis class $$\mathcal{F}$$ from which we learn $$f$$.

#### Assumption (Choice) 1: Linear Hypothesis Class / Linear Regression Under Squared Loss

The loss is then 

$$ \begin{equation}
L(f) = L(w) = \mathbb{E}[(w^Tx-y)^2]
\end{equation} $$

As a shorthand let $$w^* \in argmin_{w} L(w)$$

##### Lemma 2: For linear models, we can separate out the error inherent in linear models and error incurred from a finite sample

$$ \begin{equation}
L(\hat{w}) = \mathbb{E}_{x} [ Var(y \vert x) ] + \mathbb{E}_{x} [ (\mathbb{E}[y \vert x] - {w^{*}}^{T} x)^{2} ] + \mathbb{E}_{x}[({w^{*}}^{T} - \hat{w}^{T} x)^{2} ]
\end{equation} $$

The second term can be thought of as approximation error inherent in linear models, and the thrid term is estimation error for a finite sample. So squared linear regression error equals intrinsic variable error plus linear model error plus finite approximation error.

#### Parameterized families of hypotheses

In general, a parameterized family is given by a parameter space. For each $$\theta$$ in the space, there exists a hypothesis, written $$f_{\theta}(x)$$ or $$f(\theta;x)$$.

##### Well-specified case and maximum likelihood

In the well specified case, we can say that $$y \vert x \sim P_{\theta^*} (y \vert x)$$ is distributed according to some ground truth parameter $$\theta^*$$. Our hypothesis space is $$P_{\theta}(y \vert x)$$, a family of distributions parameterized by $$\theta \in \Theta$$.

##### Metric 2: Maximum Likelihood Loss Function

(Note that metric 1 was squared loss.)

Define the maximum likelihood loss function by

$$ \begin{equation}
\mathcal{l}((x,y),\theta) = - \log P_{\theta}(y \vert x)
\end{equation} $$

So minimizing the loss function is equivalent to maximizing the likelihood of the data. As an example, let $$y \vert x$$ by a Gaussian distributed with mean $${\theta^{*}}^{T} x$$ and variance $$1$$, i.e. suppose $$y \vert x \sim N({\theta^{*}}^{T} x,1)$$. The likelihood loss is then

$$ \begin{align*}
\mathcal{l}((x,y), \theta) &=  - \log P_{\theta} (y \vert x) \\
&= - \log exp \big( - \dfrac{(y - \theta^{x})^{2}}{2} \big) + c \\
&= \dfrac{(y - \theta^{x})^{2}}{2} + c
\end{align*} $$

Where $$c$$ is the log of the normalizing constant. 

##### Training Loss

We need to approximate the training loss using samples since often we do not know the true underlying distribution $$P$$. Define the training loss as

$$
\hat{L}(\theta) = \frac{1}{n} \sum_{i=1}^n \mathcal{l}((x^{i},y^{i}), \theta)
$$

In the special case of maximum likelihood, we plug in to get the *sample maximum likelihood loss* aka *training loss*

$$
\begin{equation}
\hat{L}(\theta) = - \frac{1}{n} \sum_{i=1}^n \log p_{\theta}(y \vert x)
\end{equation}
$$

And define the *maximum likelihood estimator*

$$ \begin{equation}
\hat{\theta}_{MLE} \in argmin_{\theta \in \Theta} \hat{L}(\theta)
\end{equation} $$

This approximation is "good" in the asympotic sense. Below, we quantify this statement.

#### Theorem 1: Asymptotic of MLE

##### Assume

1. That $$\nabla^2 L(\theta^*)$$ is full rank, i.e. the Hessian of the loss is a full rank matrix
2. $$\hat{\theta} = \theta_{MLE}$$ is a consistent estimator. (As the number of data points, $$n$$, approaches infinity, the minimizer of the training loss $$\hat{\theta}_{MLE}$$ converges to the true maximum likelihood parameter $$\theta^*$$, i.e. the true $$P_{\theta^*}(y \vert x)$$.)

##### Let
$$ \begin{equation}
Q = \mathbb{E}_{(x,y) \sim P} [\nabla_{\theta}(\log p_{\theta}(y \vert x))(\theta^*) \nabla_{\theta}(\log p_{\theta}(y \vert x))(\theta^*)^T].
\end{equation} $$
I.e. $$Q$$ is the Fisher information matrix

##### Then

1. $$\sqrt{n}(\hat{\theta} - \theta^*) \rightarrow N(0, Q^{-1})$$

##### Interpreting Theorem 1

Thoerem 1 basically says that the bias of the MLE converges in distribution to a Gaussian. The $$\sqrt{n}$$ term indicates that the bias of the MLE is 0 up to order $$\frac{1}{\sqrt{n}}$$. (Interestingly, we can lower the bias of the MLE using second-order methods, which we'll comment on in future posts.)

To fully interpret Theorem 1, let's unpack what it means for an estimate of a model parameter to converge in distribution to the true parameter value. 

Stepping back to see the bigger picture, remember that the general paradigm of ML/optimization iteratively updates a guess of $$\theta$$. This forms a sequence of $$\hat{\theta}_{1}, \hat{\theta}_{1},...,\hat{\theta}_{k}, \hat{\theta}_{k+1}$$ which "converge" to the true value $$\theta^{*}$$. Because the process we use to estimate $$\theta$$ is stochastic, i.e. our available data is observed distributionally, and the data inform the estimates $$\hat{\theta}_{i}$$, each $$\hat{\theta}_{i}$$ is a random variable distributed according to $$d_{\hat{\theta}_{i}}$$. 

The maximum likelihood estimator has many attractive properties, among them being *consistency*, as we mentioned in our assumptions. Consistency says that $$d_{\hat{\theta}_{1}}, d_{\hat{\theta}_{2}}, d_{\hat{\theta}_{3}}, ...$$ become more concentrated, or spiked, around the true parameter value. 

![consistent](/files/consistent_estimator.png)
*source: Wikipedia*

In general, concentration bounds allow us to quantify how spiked these distributions become around the central value. Theorem 1, which states that the bias is converges to a mean-zero Gaussian, is an example of a statement that quantifies this convergence process. Next time, we prove Theorem 1. 

### Resources

1. CS229T notes by Percy Liang and Tengyu MA
2. [https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation#Properties)
3. [https://en.wikipedia.org/wiki/Consistent_estimator](https://en.wikipedia.org/wiki/Consistent_estimator)

### Next

1. Proof of the Asymptotics of MLE, touching on the conditions for an estimator to be consistent
