---
layout: post
title: Expectation Maximization, Variational Inference, Majorization Minimization, TRPO
published: true
usemathjax: true
tags: machine learning
---

The EM algorithm is a generic optimization algorithm for cases where latent variables make the optimization problem difficult. It is an alternating optimization
algorithm based on Jensen's inequality. This post will be concerned with deriving the EM algorithm from first principles, exploring use cases, such as optimizing mixtures of Gaussians, relating it to variational inference using the reparameterization trick, and showing how it is conceptually similar to Trust Region Policy Optimization.

To begin, let's review some definitions and theorems.
<!--excerpt-->

#### Convexity

$$\begin{equation}
f \text{ is convex } \leftrightarrow f''(x) \ge 0 \text{  } \forall x \in \text{domain}
\end{equation}$$

If the domain of $$x$$ is multivariate this is equivalent to saying that the Hessian $$H$$ of $$f$$ is positive semi-definite. If f'' is positive at all $$x$$, then 
$$f$$ is strictly convex.

#### Theorem: Jensen's Inequality

If $$f$$ is convex and $$X$$ is a random variable, then 

$$ \begin{equation}
\mathbb{E}[f(X)] \ge f(\mathbb{E}[X])
\end{equation} $$

When $$f$$ is strictly convex, then equality occurs iff $$E[X] = X$$, i.e. the random variable $$X$$ is a constant. Jensen's says that, for $$f$$ convex, the mean of $$f(x)$$ is lower bounded by $$f$$ applied to the mean of $$X$$.

#### Deriving the EM Algorithm

Say that we have a latent variable model $$p(x,z;\theta)$$ with $$z$$ being the latent variable. Since we only have access to $$x$$, we can rewrite the model as the density for $$x$$ by marginalizing over $$z$$.

$$ \begin{equation}
p(x; \theta) = \sum_{z} p(x,z;\theta)
\end{equation} $$

And fit the parameters of the model, $$\theta$$, by maximizing the log-likelihood of the data $$x$$,

$$ \begin{align*}
l(\theta) &= \sum_{i=1}^n \log p(x^{(i)}; \theta) \\
&= \sum_{i=1}^n \log \sum_{z^{(i)}} p(x^{(i)}, z^{(i)}; \theta) \\
\end{align*} $$

Oftentimes, when the latent $$z^{(i)}$$'s are fully observed, the $$MLE$$ is easy. In the latent variable setting, $$EM$$ is an efficient $$MLE$$ algorithm. The strategy of the $$EM$$ algorithm is to construct a lower bound (E-step) and then optimize the lower bound (M-step). To derive the EM algorithm, let's restrict our attention to the single data point case, where we are optimizing $$\log p(x)$$.

$$ \begin{align*}
\log p(x; \theta) &= \log \sum_{z} p(x,z;\theta) \\
&= \log \sum_{z} Q(z) \dfrac{p(x,z;\theta)}{Q(z)} \\
& \ge \sum_{z} Q(z) \log \dfrac{p(x,z;\theta)}{Q(z)} \\
\end{align*} $$

Where we apply Jensen's equality to $$log$$, a concave function. (The direction of Jensen's is flipped for concave fucntions.)

The lower bound works for arbitrary distributions $$Q$$ but we would like the select $$Q$$ such that the bound is tight, thereby giving us a more accurate estimate.
Note that $$\sum_{z} Q(z) \log \dfrac{p(x,z;\theta)}{Q(z)}$$ is the form of an expectation. For the bound to be tight, we simply need $$\dfrac{p(x,z;\theta)}{Q(z)}$$ to be a constant random variable, so we force this by choosing

$$ \begin{equation}
Q(z) \propto p(x,z;\theta)
\end{equation} $$

Giving us $$\dfrac{p(x,z;\theta)}{Q(z)} = C$$ and 

$$ \begin{align*}
Q(z) &= \dfrac{p(x,z;\theta)}{\sum_{z} p(x,z;\theta)} \\
&= \dfrac{p(x,z;\theta)}{p(x;\theta)} \\
&= p(z \vert x;\theta) \\
\end{align*} $$

To simplify our notation going forward, we let

$$ \begin{equation}
ELBO(x;Q,\theta) = \sum_{z} Q(z) \log \dfrac{p(x,z;\theta)}{Q(z)}
\end{equation} $$

The EM algorithm alternates between:

(a) setting $$Q(z0 = p(z \vert x; \theta)$$

(b) maximizing $$ELBO(x;Q,\theta)$$ wrt $$\theta$$ while $$Q$$ is fixed

#### Deriving the EM Update for Mixtures of Gaussians

We'll formally derive the update for a covariance matrix, noting that the update for a mean vector is similar. To maximize ELBO wrt $$\Sigma_{l}$$, do

$$ \begin{align*}
\nabla_{\Sigma_{l}} \sum_{i=1}^n \sum_{j=1}^k w_{j}^{(i)} \log \dfrac{1}{(2 \pi)^{d/2} \vert \Sigma_{j} \vert^{1/2}} exp(\dfrac{\frac{-1}{2} (x^{(i)} - \mu_{j})^{T} \Sigma_{j}^{-1} (x^{(i)} - \mu_{j}) \cdot \phi_{j}}{w_{j}^{(i)}}) &= \\
&=  \nabla_{\Sigma_{l}} \sum_{i=1}^n \sum_{j=1}^k w_{j}^{(i)} \big[  \log \dfrac{1}{(2 \pi)^{d/2} \vert \Sigma_{j} \vert^{1/2}} + \log \phi_{j} - \log w_{j}^{(i)} - \dfrac{1}{2} (x^{(i)} - \mu_{j})^{T} \Sigma_{j}^{-1} (x^{(i)} - \mu_{j})  \big] \\
&=  \sum_{i=1}^n w_{l}^{(i)} \big[ \nabla_{\Sigma_{l}} \log \dfrac{1}{(2 \pi)^{d/2} \vert \Sigma_{j} \vert^{1/2}} -  \nabla_{\Sigma_{l}} \dfrac{1}{2} (x^{(i)} - \mu_{j})^{T} \Sigma_{j}^{-1} (x^{(i)} - \mu_{j}) \big] \\
&= \sum_{i=1}^n  w_{l}^{(i)} \big[ \dfrac{-1}{2} Sigma_{l}^{-1} + \dfrac{1}{2} \Sigma_{l}^{-T} (x^{(i)} - \mu_{j})(x^{(i)} - \mu_{j})^{T} \Sigma_{l}^{-T} \big] \\
\end{align*} $$

Setting to $$0$$ and solving gives the update

$$\Sigma_{l} = \dfrac{\sum_{i=1}^n w_{l}^{(i)} (x^{(i)} - \mu_{j})(x^{(i)} - \mu_{j})^{T}}{\sum_{i=1}^n w_{l}^{(i)} }$$. We used a couple of identities from the Matrix Cookbook to help derive our update. For example, $$\frac{d}{d \Sigma} a^{T} \Sigma^{-1} a = - \Sigma^{-T} a a^T \Sigma^{-T}$$. Futhermore, we assume orthogonality of the covariance matrix, giving us $$\Sigma_{l}^{-T} = \Sigma_{l}^{-1}$$.

The update for $$\phi_{j}$$ is derived in Tengyu Ma and Andrew Ng's [notes](http://cs229.stanford.edu/notes2020spring/cs229-notes8.pdf)

#### Variational Inference

The term Variational Auto-Encoder generally refers to a family of algorithms that extend the EM algorithms to models parameterized by neural networks. For many datasets, VAE does not achieve top performance but is important because it demonstrates central ideas about how to extend EM algorithms to high-dimensional continuous models with latent variables. Specifically, VAE extends the techniques of variational inference using the "reparameterization trick." 

Recall previously that we had to make a choice for the sampling distribution of the ELBO bound, $$Q(z)$$. Specifically, choosing $$Q(z) = p(z \vert x;\theta)$$ gave us a tight ELBO bound to optimize against. Although this worked for Mixture of Gaussians, in many complex models it's intractable to compute $$p(z \vert x;\theta)$$ exactly. Since ELBO works for arbitrary $$Q$$, we instead use an approximation of the true posterior distribution.

Let $$\mathcal{Q}$$ be a family of $$Q$$. 

![q_continuum](/files/QC2.jpeg)

Now, write the objective

$$
\max_{Q \in \mathcal{Q}} \max_{\theta} ELBO(Q,\theta)
$$

We simply re-interpret the un-marginalized distribution. Note that we are no longer fixing $$Q$$ in the ELBO optimization, as we did before in EM. The main question now is of what structural assumptions we should make on $$Q$$ so that we can efficiently maximize the objective. 

In the case that $$z$$ is high dimensional and discrete, one popular assumption is the mean field assumption, i.e. that. $$Q_{i}(z)$$ is a distribution with independent coordinates. I.e. that we can do the decomposition $$Q_{i}(z) = Q_{i}^{1}(z)...Q_{i}^{k}(z)$$. 

Moving forward with the mean field assumption, we still need a succinct and computationally useful way to represent $$Q_{i}$$. A natural choice is to assume $$Q_{i}$$ is a diagonal gaussian such that

$$ \begin{equation}
Q_{i}(z^{(i)}) \approx p(z^{(i)} \vert x^{(i)}; \theta)
\end{equation} $$

Let $$q(\cdot;\phi)$$ and $$v(\cdot;\psi)$$ be two functions that map from $$\mathbb{R}^{d}$$ to $$\mathbb{R}^{k}$$ and set

$$ \begin{equation}
Q_{i} = \mathcal{N}(q(x^{(i)};\phi),diag(v(x^{(i)};\psi))^2)
\end{equation} $$

And rewrite the ELBO as

$$ \begin{equation}
ELBO(\phi, \psi, \theta) = \sum_{i=1}^n \mathbb{E}_{z^{(i)} \sim Q_{i}} \big[ \log \dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_{i}(z^{(i)})} \big]
\end{equation} $$

Where $$Q_{i} = \mathcal{N}(q(x^{(i)};\phi),diag(v(x^{(i)};\psi))^2)$$. Instead of computing the maximum analytically, as with the mixture of gaussians, we use gradient ascent on $$\theta, \phi, \psi$$. 

Computing the gradient over $$\phi$$ and $$\psi$$ is tricky because $$z_i \sim Q_{i}$$ depends on them, and we're taking the expectation over $$z_i \sim Q_{i}$$. This is where the re-parameterization trick comes in handy. The main idea is that we rewrite $$z_i$$ in terms of $$x_i$$, which we have access to. So, rewrite $$z^{(i)} \sim Q_{i} = \mathcal{N}(q(x^{(i)};\phi),diag(v(x^{(i)};\psi))^2)$$ as

$$ \begin{equation}
z^{(i)} = q(x^{(i)};\phi) + v(x^{(i)};\psi) \odot \xi^{(i)}
\end{equation} $$
Where $$\xi^{(i)} \sim \mathcal{N}(0,I_{k \times k})$$

Using this re-parameterization, we have

$$ \begin{align*}
\mathbb{E}_{z^{(i)} \sim Q_{i}} \big[ \log \dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_{i}(z^{(i)})} \big] &= 
\mathbb{E}_{\xi^{(i)} \sim \mathcal{N}(0,1)} \big[ \log \dfrac{p(x^{(i)},q(x^{(i)};\phi) + v(x^{(i)};\psi) \odot \xi^{(i)};\theta)}{Q_i{q(x^{(i)};\phi) + v(x^{(i)};\psi) \odot \xi^{(i)}}} \big] \\
\end{align*} $$

And 

$$ \begin{align*}
\nabla_{\phi} \mathbb{E}_{z^{(i)} \sim Q_{i}} \big[ \log \dfrac{p(x^{(i)},z^{(i)};\theta)}{Q_{i}(z^{(i)})} \big] 

&= 
\nabla_{\phi} \mathbb{E}_{\xi^{(i)} \sim \mathcal{N}(0,1)} \big[ \log \dfrac{p(x^{(i)},q(x^{(i)};\phi) + v(x^{(i)};\psi) \odot \xi^{(i)};\theta)}{Q_i{q(x^{(i)};\phi) + v(x^{(i)};\psi) \odot \xi^{(i)}}} \big] \\
&= 
\mathbb{E}_{\xi^{(i)} \sim \mathcal{N}(0,1)} \big[ \nabla_{\phi} \log \dfrac{p(x^{(i)},q(x^{(i)};\phi) + v(x^{(i)};\psi) \odot \xi^{(i)};\theta)}{Q_i{q(x^{(i)};\phi) + v(x^{(i)};\psi) \odot \xi^{(i)}}} \big] \\
\end{align*} 
$$

Which shows we can estimate the gradient by sampling $$\xi_{(i)}$$ in lieu of $$z_{(i)}$$!

#### Majorization Minimization

The EM algorithm is an example of a broader class of algorithms called Majorization Minimization algorithms. A function $$g(\theta \vert \theta^n)$$ is said to majorize the function $$f(\theta)$$ at $$\theta^n$$ given

$$ \begin{equation}
f(\theta^n) = g(\theta^n \vert \theta^n)
\end{equation} $$
$$ \begin{equation}
f(\theta^n) \le g(\theta^n \vert \theta^n)
\end{equation} $$
for all $$\theta$$.

In minimization, we choose a majorizing function $$g(\theta \vert \theta^n)$$ and minimize it. This produces the next point $$\theta^{n+1}$$ in the algorithm. MM algorithms are quite stable because of guaranteed monotonic improvement, or, as the stat community might say, because of the MM descent property $$f(\theta^{n+1}) \le f(\theta^{n})$$, which is fulfilled with strict inequality unless both

$$
g(\theta^{n+1} \vert \theta^{n}) = g(\theta^{n} \vert \theta^{n})
$$ and

$$ 
f(\theta^{n+1}) = g(\theta^{n+1} \vert \theta^{n})
$$

which follows from

$$ \begin{align*}
f(\theta^{n+1}) &= g(\theta^{n+1} \vert \theta^{n}) + f(\theta^{n+1}) - g(\theta^{n+1} \vert \theta^{n}) \\
&= \le g(\theta^{n} \vert \theta^{n}) + f(\theta^{n}) - g(\theta^{n} \vert \theta^{n}) \\
&= f(\theta^{n}) \\
\begin{align*} $$

#### Connection to TRPO

TRPO is an example of Minorization Maximization. Referring back to [this](https://annhe.xyz/2021/04/12/policy-gradients/), the value function $$V^{\pi_{n}}$$ is minorized by the pseudo-objective $L_{\pi_{n}}$. In TRPO maximization, we produce $$\pi_{n+1}$$ by maximizing $L_{\pi_{n}}$.

#### Resources

1. [MM](https://www.stat.berkeley.edu/~aldous/Colloq/lange-talk.pdf)
2. [CS 229 Notes by Tengyu Ma and Andrew Ng](http://cs229.stanford.edu/notes2020spring/cs229-notes8.pdf)
3. Numerical Optimization by Nocedal and Wright

##### For next time(s):

1. TRPO implementation with derivation of conjugate gradient descent and line search
2. VAE as the foundation of transformers and language models
3. KL divergence interpretation of EM
4. EM for mixtures of gaussians with implementation


