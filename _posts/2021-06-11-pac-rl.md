---
layout: post
title: Proof of sublinear regret of UCB algorithms for bandits
published: true
usemathjax: true
tags: reinforcement learning, theory
---

This post will be concerned with the proof that the UCB algorithm achieves sublinear regret for multi-armed bandits. First, let's set up the preliminaries of the problem. 

UCB and bandits are part of fast reinforcement learning, a problem setting that is concerned with making sample efficient decisions, especially in applications where experience is costly or difficult to achieve. Bandits are a simplified version of an MDP--in particular one with only one state. We have a finite set of actions $$\mathcal{A}$$, each of which induces a reward distribution $$\mathcal{R}^{a}(r) = P(r\middlea)$$. At each step, the agent selects some $$a_{t} \in \mathcal{A}$$ and the overall goal is to maximize the cumulative reward, $$\sum_{t=1}^T r_{t}$$, or equivalently, to minimize the cumulative regret, $$l_{T} = \mathbb{E}[\sum_{t=1}^T V^{*} - Q(a_{t})]$$.

Here we define:

* Action-value: $$Q(a) = \mathbb{E}[r|a]$$
* Optimal value: $$V^{*} = Q(a^{*}) = \max_{a} Q(a)$$

#### Lower Bound

Lai and Robbins in 1985 [proved](https://www.stat.berkeley.edu/~bartlett/courses/2014fall-cs294stat260/lectures/bandit-lower-bound-notes.pdf) that the asymptotic total regret is at least logarithmic in the number of steps. The lower bound gives a measure of the inherent difficulty of the problem, and establishes a goal to shoot for when designing algorithms. We make a sidenote that the performance of any bandit algorithm is determined by how similar the optimal arm is to other arms. Similarly, how difficult it is to distinguish an optimal arm from a suboptimal arm. Thus, hard bandit problems have similar looking, i.e. small $$D_{KL}(\mathcal{R}^{a} || \mathcal{R}^{a^{*}})$$, arms with different means. 

#### Optimism in the Face of Uncertainty

A high level heuristic we use to solve this problem is optimism under uncertainty. We repeatedly choose actions that look like they have high value. Why? Either the action does turn out to be good, and we reap a good reward, or the action has low reward, and we update our knowledge of it, leading us to choose it less over time (minimizing cumulative regret).

For each arm $$a$$, we estimate an upper confidence bound $$U_{t}(a)$$, such that with high probability, $$Q(a) \le U_{t}(a)$$. I.e., with high confidence, our estimate is an upper bound on the true arm value. The algorithm then, at each time step, selects the action with maximum UCB.

At each time step, as long as all of the UCB bounds simultaneously hold, we're in good shape to prove sublinear regret. Why? Let's first do a proof-sketch so we can get a birds-eye view of how this will go. 

If all UCB bounds hold, then no matter what action, $$a_{t}$$, we take, we have that $$U_{t}(a_{t}) > Q(a^{*})$$. There are two cases. 

Case 1: $$a_{t} = a^{*}$$

The action we select is actually the optimal action. Then $$U_{t}(a_{t}) = U_{t}(a^{*}) > Q(a^{*})$$.

Case 2: $$a_{t} \ne a^{*}$$

Then $$U_{t}(a_{t}) > U_{t}(a^{*}) > Q(a^{*})$$.

To see how this is relevant, let's look at the outline of the regret proof.

\begin{align*}
regret(UCB,T) &= \sum_{t=1}^{T} (Q(a^{*}) - Q(a_{t})) \\
&= \sum_{t=1}^{T} U_{t}(a_{t}) - Q(a_{t}) + Q(a^{*}) - U_{t}(a_{t}) \\
& \le \sum_{t=1}^{T} U_{t}(a_{t}) - Q(a_{t}) 
\end{align*}

We have the inequality on the third line because the UCB bounds give us $$Q(a^{*}) - U_{t}(a_{t}) \le 0$$. Our estimate of $$U_{t}(a_{t})$$ will be of the form $$U_{t}(a_{t}) = \hat{Q}(a_{t}) + d$$ where $$\sum_{t=1}^T d_{t}$$ will be a sublinear term. Ok, now onto the details.

#### Chernoff-Hoeffding Bound


