---
layout: post
title: How structured do PCP queries need to be?
published: false
usemathjax: true
---

> Topics touched on: probabilistically checkable proofs, discrete random processes, probabilistic method, exponential time hypothesis, lower bounds, random set cover, PAC learning theory

![xkcd](/assets/xkcd_proof.png)

Just as the Turing Machine provides an abstract lens through which we can study computation in general, interactive proofs provide a template through which theorists can characterize specific classes of computational problems. In the most basic form, interactive proofs can be thought of as a game between two parties, a prover and a verifier. The prover and verifier are each Turing machines which perform their own computations. The interactive proof model allows the two parties to communicate, and the behavior of one may alter the behavior of the other. <!--excerpt-->

Often times, the kinds of protocols that people design interactive proofs are for proofs of membership. That is, the prover knows that some string x belongs to some set L and wants to convince the verifier to believe that this is so, i.e. that the statement "x belongs to the language L" is true. In general, this set L can be almost anything - it can be the set of graphs that are 3-colorable, the set of descriptions of Turing Machines which halt when run on their own description, or (for the sake of imagination) the set of true mathematical statements.

Formally, a language L in computer science is defined with respect to an alphabet, Σ, which is a set of (usually finite) symbols. A language L is simply a subset of the possible strings that can be formed using the symbols in Σ.

As anyone who has taken a proof-based math class can tell you, verifying a chain of arguments purportedly convincing you of the veracity of statement is an arduous task. There are so many things that could go wrong - a mishap of calculation, a faulty act of modus ponens, starting from wrong assumptions - among others. If there is a mistake, you must catch any instance of such a mistake, but you do not know a priori what the mistake will be or where in the proof it will occur. In this view, the verifier must carefully read, line by line, a proof, "remembering" what it has seen before, in order to catch an inconsistency.

If P is the class of languages that can be solved efficiently (the set of languages for which membership can be decided efficiently), then class NP (of P vs. NP fame) is exactly those languages which have proofs of membership which can be verified efficiently.

The class NP is often defined with respect to the notion of a witness. A language L is said to be in NP if for every string x ∈ L, there exists another string w, not much longer than x, that bears witness to this fact. And for every x ∉ L, no such string (longer than x by a polynomial factor), can convince the NP verifier otherwise.

The non-interactive proof protocol for an NP goes as follows. The prover and verifier are each given x as inputs, but the prover additionally is given the witness for x, w. The prover sends w directly to the verifier. The verifier checks w to decide whether x ∈ L, typically in the process reading the entire proof, w.

(NP languages have protocols with perfect completeness and perfect soundness.) [more explanation here]

The existence of a type of proof called a probabilistically checkable proof (PCP) tells us that, actually, there are certain (interesting!) languages which have proofs that can be checked by reading just a few spots in the proof. For certain problems, including graph 3-colorability, membership can be verified in a "lazy" way. That is, given a NP language L and a statement x, I have a generic way of reading just three bits of any alleged proof and decide with high confidence whether x ∈ L.

Instead of thinking about the interaction between a prover and a verifier, we focus primarily on a verifier, who now has oracle access to a proof string π. The verifier has arbitrary access to a string which encodes the proof π that some x belongs to the language L. On input x, the verifier tosses some r(n) coins to decide some q(n) indices to read the proof at. In the course of its execution, the verifier is allowed to do arbitrary computations as long as its running time is bounded by t(n). At the end, the verifier outputs a single bit deciding the status of x.

![eli](/assets/eli_notes.png)

The PCP Theorem proved by Arora, Lund, Motwani, Sudan and Szegedy says that NP = PCP(length = poly(n), randomness = O(log(n)), query = P(1), time = poly(n)). That is, the class of efficiently verifiable languages is exactly those which have PCPs where the proof string is only polynomially longer than the input length, the (efficient) verifier flips only logarithmically many coins, and asks to read the proof at only a constant number of bits.

The particular instantiation of the PCP template used to prove the 'basic PCP Theorem' involved using error correcting codes to encode the NP witness w. Error correcting codes, originally designed for communicating over faulty channels, have the property of redundancy. This in turn amplifies any errors that occur in the original proof, w, making inconsistencies easier for the verifier to catch.

At this point, my intuition started twitching. It seemed weird that regardless of the input or witness size, you could read a fixed number of bits of a proof and be convinced of the veracity of a statement. It was basically saying that for NP, the languages which can be efficiently verified, verification only depends on a small, fixed number of junction points.

Which lead me to ask, how much knowledge does the verifier need to have of where to look? How simple can the verifier be? Can it haphazardly make any set of random queries? In particular, if the number of queries is k, can the verifier make k k-wise independent uniformly random queries?

It seems that all known PCP constructions use correlated queries in the verifier to achieve the soundness requirement (so that the verifier can catch a cheating prover trying to claim some x ∉ L is otherwise.)

For example, the Hadamard PCP uses three queries x, y, z. The first two are uniformly and independently chosen, but the third is z = x+y.
But what specifically happens if we let x,y,z be 3-wise independent? It turns out that you get results that are in contradiction with certain strongly believed conjectures in computer science, in particular we get that co-RP = NP.

> In complexity theory, folks are concerned with classifying the inherent difficulty of computational problems on fundamental measures like time and space. There's an entire [hierarchy](https://simons.berkeley.edu/news/research-vignette-tan-rossman-2015) of more and more difficult problems, and many practical things depend on the separation of classes within this hierarchy. [For example](http://people.cs.uchicago.edu/~fortnow/papers/pnp-cacm.pdf), the security of public key cryptography (i.e. the infrastructure of the internet) depends on $$P \ne NP$$. $$co-RP \ne NP$$ is a similar such statement, and is conjectured to [hold](https://cstheory.stackexchange.com/questions/47337/implications-of-proving-np-rp-on-complexity-theory). Every time someone writes a paper stating otherwise, people get pretty [excited](https://www.reddit.com/r/math/comments/i3fbl6/rp_np_according_to_a_new_paper_published_by_a/). Because these conjectures are widely accepted, people can prove statements in complexity theory by showing that the contradiction would invalidate one of the conjectures. $$co-RP \ne NP$$ is implied by a conjecture called the [exponential time hypothesis](https://en.wikipedia.org/wiki/Exponential_time_hypothesis).

If the PCP verifier can get away with uniformly random k-wise independent queries to the proof string, then we are basically saying that what matters is the relative frequency of symbols in the string and not their order. Restricting our attention to bit strings, all the PCP verifier needs to know in order to decide whether x ∈ L is a histogram of the number of 0s and the number of 1s in the string. We can use this observation to construct an RP machine to decide an arbitrary NP language, thus proving RP = NP (as RP is already known to be contained in NP.)

We essentially go about the business of guessing what the correct distribution of 0s and 1s should be for a valid proof string π of x ∈ L. Since π is only polynomial in length, and we are dealing with bit strings, then there are only polynomially many possible histograms, and we can try them all with only a polynomial overhead in running time. (To see that there are only polynomially many histograms, note that the histogram is completely determined by the number of 0's present in a proof string. If your proof string is of length N, then the number of 0's can have is between 0 and N, giving N+1 histograms.)

![histogram](/assets/histogram.png)

### To-Do:

Explain co-RP as a class

For a language L ∈ NP, we construct an RP machine M that decides L. Since NP = PCP(poly(n), log(n), O(1)), there is a PCP verifier V that reads polynomial length proofs and makes a constant number, k, of queries to the proof oracle. Our machine M will act as the proof oracle and answer any oracle queries V makes. On input x, M will iterate through every possible histogram for the proof π. M will run V on this proof, meaning M will run V, and every time V makes a proof query, M will simulate the answer by flipping a biased coin that has the exact bias as dictated by the histogram of π. If for any histogram, V accepts, then M accepts. By completeness of the PCP protocol, if x ∈ L, then there exists a histogram on which V will accept, and thus if x ∈ L, M will accept with probability 1. On the contrary, if x ∉ L, by soundness of the PCP protocol, no matter what the histogram is, V accepts with probability less than a. Thus, for x ∉ L, the RP machine M will reject with probability at least 1 - p(n) ⋅ a. 

Here, we have assumed that the alphabet we operate on is constant in size. This is important for the sparsity of possible histograms that summarize the possible proof strings for the PCP verifier, though it is possible to think about computation on polynomial-sized alphabets. 
When we move to polynomial-sized alphabets, we lose the property of sparsity of possible histograms that summarize all possible proof strings. If the proof string is of length N = p₁(n) and the alphabet size is k = p₂(n), then the number of possible histograms is equivalent to choosing N objects from a set of size k, with replacement--which in particular is bounded below and above exponentially,

$$\big( \dfrac{k + N - 1}{N} \big)^N \le \big( \binom{k + N - 1}{N} \big) \le \big( \dfrac{e(k+N-1)}{N} \big)^N$$

In other words, if we follow the same strategy, we would incur an exponential runtime. However, we can still achieve similar complexity theoretic statements as with the constant alphabet version.

The trick is to use an approximate set cover. Basically, we summarize groups of similar proof strings by a representative proof string. Then instead of checking all possible histograms, we check only the histograms of the set of representative strings. The important part is that our set of representative strings (a) covers the original set well and (b) is polynomial in size.

![set_cover](/assets/set_cover.png)

What do we mean exactly by "covers the original set well?" Our goal is to construct a polynomial-time algorithm to decide $$L$$ guaranteed to accept on $$x \in L$$ and with error $$\le \frac{1}{2}$$ on $$x \not \in L$$, so we can show the impossibility of query-efficient uniformly random query PCPs on polynomial size alphabets. 

Suppose that we start with a PCP system in which the verifier accepts on $$x \not \in L$$ with probability less that or equal to some constant $$0 < s \le \frac{1}{2}$$. And this PCP system has some set of proof strings $$\mathcal{S}$$. Then, we want to produce a set cover of the proof strings using random strings, $$\mathcal{S}'$$, such that on this new set $$\mathcal{S}'$$ we can verify $$x \in L$$ with probability $$1$$ and we will still err on $$x \not in L$$ with probability $$\le \frac{1}{2}$$. If we can produce a set $$\mathcal{S}'$$ of strings that are statistically close to the original set, then we can argue that our new proof system using $$\mathcal{S}'$$ is sound by appealing to the soundness of the original PCP system. If the verifier accepts with high probaiblity on the random histogram, then there exists a fixed histogram on which the verifier accepts with high probaiblity. In complexity theory, we call this an averaging argument, though I believe it is an instance of the more generic probabilistic method.



### To-Do:

Explain probabilistic method

### To-Do:

Similarity between two random processes... Visualization?

### To-Do:

Explain the connection to PAC learning theory
