---
layout: post
title: Fast and PAC Reinforcement Learning
published: false
usemathjax: true
tags: reinforcement learning, theory
---

### Proof of sublinear regret of UCB algorithms

This post will be concerned with the proof that the UCB algorithm achieves sublinear regret for multi-armed bandits. First, let's set up the preliminaries of the problem. 

UCB and bandits are part of fast reinforcement learning, a problem setting that is concerned with making sample efficient decisions, especially in applications where experience is costly or difficult to achieve. Bandits are a simplified version of an MDP--in particular one with only one state. We have a finite set of actions $$\mathcal{A}$$, each of which induces a reward distribution $$\mathcal{R}^{a}(r) = P[r|a]$$. At each step, the agent selects some $$a_{t} \in \mathcal{A}$$ and the overall goal is to maximize the cumulative reward, $$\sum_{t=1}^T r_{t}$$, or equivalently, to minimize the cumulative regret, $$l_{T} = \mathbb{E}[\sum_{t=1}^T V^{*} - Q(a_{t})]$$.

Here we define:

* Action-value: $$Q(a) = \mathbb{E}[r|a]$$
* Optimal value: $$V^{*} = Q(a^{*}) = \max_{a} Q(a)$$

#### Lower Bound

Lai and Robbins in 1985 [proved](https://www.stat.berkeley.edu/~bartlett/courses/2014fall-cs294stat260/lectures/bandit-lower-bound-notes.pdf) that the asymptotic total regret is at least logarithmic in the number of steps. The lower bound gives a measure of the inherent difficulty of the problem, and establishes a goal to shoot for when designing algorithms. We make a sidenote that the performance of any algorithm is determined by how similar the optimal arm is to other arms. Similarly, how difficult it is to distinguish an optimal arm from a suboptimal arm. Thus, hard bandit problems have similar looking, small $$D_{KL}(\mathcal{R}^{a} || \mathcal{R}^{a^{*}})$$, arms with different means. 
