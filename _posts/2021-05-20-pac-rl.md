---
layout: post
title: Fast and PAC Reinforcement Learning
published: false
usemathjax: true
tags: reinforcement learning, theory
---

### Proof of sublinear regret of UCB algorithms

This post will be concerned with the proof that the UCB algorithm achieves sublinear regret for multi-armed bandits. First, let's set up the preliminaries of the problem. 

UCB and bandits are part of fast reinforcement learning, a problem setting that is concerned with making sample efficient decisions, especially in applications where experience is costly or difficult to achieve. Bandits are a simplified version of an MDP--in particular one with only one state. We have a finite set of actions $$\mathcal{A}$$, each of which induces a reward distribution $$\mathcal{R}^{a}(r) = P[r|a]$$. At each step, the agent selects some $$a_{t} \in \mathcal{A}$$ and the overall goal is to maximize the cumulative reward, $$\sum_{t=1}^T r_{t}$$, or equivalently, to minimize the cumulative regret, $$l_{T} = \mathbb{E}[\sum_{t=1}^T V^{*} - Q(a_{t})]$$.

Here we define:

* Action-value: $$Q(a) = \mathbb{E}[r|a]$$
* Optimal value: $$V^{*} = Q(a^{*}) = \max_{a} Q(a)$$

#### Lower Bound

Lai and Robbins in 1985 [proved](https://www.stat.berkeley.edu/~bartlett/courses/2014fall-cs294stat260/lectures/bandit-lower-bound-notes.pdf) that the asymptotic total regret is at least logarithmic in the number of steps. The lower bound gives a measure of the inherent difficulty of the problem, and establishes a goal to shoot for when designing algorithms. We make a sidenote that the performance of any algorithm is determined by how similar the optimal arm is to other arms. Similarly, how difficult it is to distinguish an optimal arm from a suboptimal arm. Thus, hard bandit problems have similar looking, small $$D_{KL}(\mathcal{R}^{a} || \mathcal{R}^{a^{*}})$$, arms with different means. 

#### Optimism in the Face of Uncertainty

A high level heuristic we use to solve this problem is optimism under uncertainty. We repeatedly choose actions that look like they have high value. Why? Either the action does turn out to be good, and we reap a good reward, or the action has low reward, and we update our knowledge of it, leading us to choose it less over time (minimizing regret).

For each arm $$a$$, we estimate an upper confidence bound $$U_{t}(a)$$, such that with high probability, $$Q(a) \le U_{t}(a)$$. I.e., with high confidence, our estimate is an upper bound on the true arm value. The algorithm then, at each time step, selects the action with maximum UCB.

At each time step, as long as all of the UCB bounds simultaneously hold, we're in good shape to prove sublinear regret. Why?
