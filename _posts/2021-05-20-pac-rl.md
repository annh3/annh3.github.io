---
layout: post
title: Fast and PAC Reinforcement Learning
published: false
usemathjax: true
tags: reinforcement learning, theory
---

### Proof of sublinear regret of UCB algorithms

This post will be concerned with the proof that the UCB algorithm achieves sublinear regret for multi-armed bandits. First, let's set up the preliminaries of the problem. 

UCB and bandits are part of fast reinforcement learning, a problem setting that is concerned with making sample efficient decisions, especially in applications where experience is costly or difficult to achieve. Bandits are a simplified version of an MDP--in particular one with only one state. We have a finite set of actions $$\mathcal{A}$$, each of which induces a reward distribution $$\mathcal{R}^{a}(r) = P[r|a]$$. At each step, the agent selects some $$a_{t} \in \mathcal{A}$$ and the overall goal is to maximize the cumulative reward, $$\sum_{t=1}^T r_{t}$$, or equivalently, to minimize the cumulative regret, $$l_{T} = \mathbb{E}[\sum_{t=1}^T V^{*} - Q(a_{t})]$$.
